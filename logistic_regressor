import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt

"""
Goal: generate a matrix of random values (w) that we will make
random data points conform to. Define global variables batch_size
and features which will be the dimensions of later matrices, and
our implementation of the sigmoid function for later use.
Note no bias term (b) in this one!!
"""
batch_size = 128
features = 5
w = np.random.randn(features,1)

#define sigmoid fn for later
def sigmoid(x): 
    p = 1/(1+np.exp(-x)) #use np.exp and not tf.exp!
    return p
    
#visualize sigmoid function
xx = np.linspace(-3,3,100)
plt.plot(xx,sigmoid(xx))
plt.show()

"""
Generate a matrix of random x values of size (batch_size,features).
Our p values are generated by running the dot product of each x
value and w through the sigmoid function. We then compare each p
value to a randomly generated sample from the distribution located
in the rand_samples matrix of size (batch_size,1). We generate the
values for y_data based on the comparison of each p value in the p
matrix to the corresponding random value in the rand_samples matrix.
If the p value is > the random value, the y_data value for the [i]th
value in the matrix is a 1, and otherwise it's a 0. We vectorize this
process by displaying the boolean (p>random_samples) as a float, since
True and False have corresponding float values.
"""
def gen_data(w, batch_size, features):
    x_data = np.random.rand(batch_size,features)
    p = sigmoid(np.dot(x_data,w))
    rand_samples = np.random.rand(batch_size,1)
    y_data = (p>rand_samples).astype('float')
    
#unvectorized implementation:
#     y_data = np.empty_like(p)
#     for i in range(batch_size): 
#         if p[i] > rand_samples[i]:
#             y_data[i] = 1
#         else:
#             y_data[i] = 0

    return x_data,y_data
    
"""
Here we're just printing out w for later reference and the output of
our data generation function to make sure it came out as anticipated.
Note that x_data is a matrix of size (batch_size,features) and y_data
is just a one-column vector of ones and zeros.
"""
print(w)
print(gen_data(w, batch_size, features))

"""
Now we're building the logistic regression and defining the graph in
TensorFlow. We create placeholder values for x and y and a variable for
w_hat. Note that instead of y_hat we have p_hat for easier understanding.
The p_hat is equal to our p values from earlier, so we run x and w_hat
through a sigmoid function again. (Note the use of tf.exp instead of np.exp
because we want this to be a Tensor object. Similar reasoning is for the
usage of tf.matmul instead of np.dot.) Our loss function is binary cross
entropy, again making sure to use TensorFlow functions instead of NumPy ones
to make sure they're Tensor objects. We use GradientDescentOptimizer and tell
it the learning rate and that we want to minimize the loss value we've already
calculated. Finally, we instantiate the global_variable_initializer object as
init.
"""
tf.reset_default_graph()

x = tf.placeholder(tf.float32, shape=(batch_size,features))
y = tf.placeholder(tf.float32, shape=(batch_size,1))

w_hat = tf.get_variable("w_hat", shape=(features,1), initializer=tf.random_normal_initializer(0,0.5))

p_hat = 1/(1+tf.exp(-tf.matmul(x,w_hat)))

loss = tf.reduce_mean(-(y*tf.log(p_hat)+(1-y)*tf.log(1-p_hat))) #binary cross entropy

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)

init = tf.global_variables_initializer()

"""
Run the Session to actually perform the logistical regression for 50000 steps.
For each iteration we use the gen_data function to generate our x_data and y_data
values and feed them into the optimizer using feed_dict. We print out the
loss every 1000 iterations. Finally when training is finished we print out
our w_hat value and w and see how well the algorithm trained.
"""
with tf.Session() as sess:
    sess.run(init)
    for i in range(50000):
        x_data, y_data = gen_data(w,batch_size,features)
        _ = sess.run(optimizer, feed_dict={x: x_data, y: y_data})
        if np.mod(i,1000) == 0:
            print(sess.run(loss, feed_dict={x: x_data, y: y_data}))
    print(sess.run([w_hat]))
    print(w)
