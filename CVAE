import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

"""
Goal: CVAEs build on VAEs in that they are the same except for the
addition of labels with each input. This makes it so that the network
inherently knows what each input inherently is and affects the latent
space distribution accordingly. We are still using KL divergence and
reconstruction loss as our loss metrics.
"""

#define parameters
batch_size = 250
features = 784
classes = 10
xavier_init = True #use xavier/glorot initialization

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) #import data

w1 = 512 #hidden layer 1 nodes
w2 = 256 #hidden layer 2 nodes
L = 20 #latent layer nodes

enc_merged = features + classes #merged encoder layer size
dec_merged = L + classes #merged decoder layer size

#activation functions
def sigmoid(x):
    z = 1/(1 + tf.exp(-x))
    return z

def sigmoid_np(x):
    z = 1/(1 + np.exp(-x))
    return z

def relu(x):
    z = np.maximum(x,0)
    return z

#create xavier initialization function
def xavier(rows,cols,n_in): 
    z = np.random.normal(size=(rows,cols),
                         scale=np.square(1/n_in))
    return z

#load/generate data
images = mnist.train.images
labels = mnist.train.labels

def gen_data(batch_size):
    
    random_rows = np.random.choice(images.shape[0],size=batch_size,replace=False)
    x_data = mnist.train.images[random_rows,:]
    y_data = mnist.train.labels[random_rows,:]
    
    return x_data, y_data

##to initialize the weights in TF with stored values, uncomment
# and put in desired filename, then initialize the weights with
# tf.constant_initializer and the value from the corresponding
# entry in the weight file

## flag for external xavier initialization
if xavier_init:
    upd = dict([('enc_w1',xavier(enc_merged,w1,enc_merged)),
               ('enc_b1',xavier(1,w1,enc_merged)),
               ('enc_w2',xavier(w1,w2,w1)),
               ('enc_b2',xavier(1,w2,w1)),
               ('mu_hat',xavier(w2,L,w2)),
               ('mu_b_hat',xavier(1,L,w2)),
               ('log_sigma_hat',xavier(w2,L,w2)),
               ('log_sigma_b_hat',xavier(1,L,w2)),
               ('dec_w1',xavier(dec_merged,w2,dec_merged)),
               ('dec_b1',xavier(1,w2,dec_merged)),
               ('dec_w2',xavier(w2,w1,w2)),
               ('dec_b2',xavier(1,w1,w2)),
               ('x_w_hat',xavier(w1,enc_merged,w1)),
               ('x_b_hat',xavier(1,enc_merged,w1))])
else:
    #upd = np.load("your_stored_weight_filename_here")
    
#random sampling
def random_sample(r):
    
    mu, log_sigma = r
    epsilon = tf.random_normal(shape=(batch_size,L))
    z = mu + ((tf.exp(0.5*log_sigma))*epsilon)
    
    return z

def random_sample_np(r):
    
    mu, log_sigma = r
    epsilon = np.random.randn(batch_size,L)
    z = mu + ((np.exp(0.5*log_sigma))*epsilon)
    
    return z

#create graph
tf.reset_default_graph()

insigma=0.001

x = tf.placeholder(tf.float32, shape=(batch_size,features)) #images
y = tf.placeholder(tf.float32, shape=(batch_size,classes)) #one-hot labels
xy = tf.concat([x,y],1) #concatenate images + labels

#encoder hidden layer 1 weight + bias
enc_w1 = tf.get_variable("enc_w1", shape=(enc_merged,w1), initializer=tf.constant_initializer(upd['enc_w1']))
enc_b1 = tf.get_variable("enc_b1", shape=(1,w1), initializer=tf.constant_initializer(upd['enc_b1']))

#encoder hidden layer 2 weight + bias
enc_w2 = tf.get_variable("enc_w2", shape=(w1,w2), initializer=tf.constant_initializer(upd['enc_w2']))
enc_b2 = tf.get_variable("enc_b2", shape=(1,w2), initializer=tf.constant_initializer(upd['enc_b2']))

#mu and sigma weights + biases
mu_hat = tf.get_variable("mu_hat", shape=(w2,L), initializer=tf.constant_initializer(upd['mu_hat']))
mu_b_hat = tf.get_variable("mu_b_hat", shape=(1,L), initializer=tf.constant_initializer(upd['mu_b_hat']))
log_sigma_hat = tf.get_variable("log_sigma_hat", shape=(w2,L), 
                                initializer=tf.constant_initializer(upd['log_sigma_hat']))
log_sigma_b_hat = tf.get_variable("log_sigma_b_hat", shape=(1,L), 
                                  initializer=tf.constant_initializer(upd['log_sigma_b_hat']))

#decoder hidden layer 1 weight + bias
dec_w1 = tf.get_variable("dec_w1", shape=(dec_merged,w2), initializer=tf.constant_initializer(upd['dec_w1']))
dec_b1 = tf.get_variable("dec_b1", shape=(1,w2), initializer=tf.constant_initializer(upd['dec_b1']))

#decoder hidden layer 2 weight + bias
dec_w2 = tf.get_variable("dec_w2", shape=(w2,w1), initializer=tf.constant_initializer(upd['dec_w2']))
dec_b2 = tf.get_variable("dec_b2", shape=(1,w1), initializer=tf.constant_initializer(upd['dec_b2']))

#output layer weight + bias
x_w_hat = tf.get_variable("x_w_hat", shape=(w1,enc_merged), initializer=tf.constant_initializer(upd['x_w_hat']))
x_b_hat = tf.get_variable("x_b_hat", shape=(1,enc_merged), initializer=tf.constant_initializer(upd['x_b_hat']))

#encode step
e_hidden1 = tf.nn.relu(tf.add(enc_b1,tf.matmul(xy,enc_w1)))
e_hidden2 = tf.nn.relu(tf.add(enc_b2,tf.matmul(e_hidden1,enc_w2)))
mu = tf.add(mu_b_hat,tf.matmul(e_hidden2,mu_hat))
log_sigma = tf.add(log_sigma_b_hat,tf.matmul(e_hidden2,log_sigma_hat))

#random sample
z = random_sample([mu,log_sigma])
merged_z = tf.concat([z,y],1)

#decode step
d_hidden1 = tf.nn.relu(tf.add(dec_b1,tf.matmul(merged_z,dec_w1)))
d_hidden2 = tf.nn.relu(tf.add(dec_b2,tf.matmul(d_hidden1,dec_w2)))
x_hat = sigmoid(tf.add(x_b_hat,tf.matmul(d_hidden2,x_w_hat)))

#calculate loss
beta = 0.001
l2 = (beta*tf.nn.l2_loss(enc_w1) +
      beta*tf.nn.l2_loss(enc_w2) +
      beta*tf.nn.l2_loss(mu_hat) +
      beta*tf.nn.l2_loss(log_sigma_hat) +
      beta*tf.nn.l2_loss(dec_w1) +
      beta*tf.nn.l2_loss(dec_w2) +
      beta*tf.nn.l2_loss(x_w_hat))
# recon_loss = tf.reduce_mean(tf.square(xy-x_hat),1)
recon_loss = tf.reduce_mean(-(xy*tf.log(x_hat)+(1-xy)*tf.log(1-x_hat)))
kl = 0.5*tf.reduce_sum(tf.exp(log_sigma)+tf.square(mu)-(1+log_sigma),1)
loss = tf.reduce_mean(tf.add(100*recon_loss,kl) + 0.1*l2)

#gradient clipping
optimizer = tf.train.AdamOptimizer(learning_rate=0.0003)
gradients = optimizer.compute_gradients(loss)
capped_gradients = [(tf.clip_by_norm(grad,5.0),var) for grad,var in gradients]
opt = optimizer.apply_gradients(capped_gradients)

#initialize variables
init = tf.global_variables_initializer()

#run Session
with tf.Session() as sess:
    sess.run(init)
    losses = []
    counter = 0
    for i in range(50000):
        x_data, y_data = gen_data(batch_size)
        _ = sess.run(opt, feed_dict={x: x_data, y: y_data})
        if np.mod(i,5000) == 0:
            counter += 1
            print("Mean Total Loss: {} ".format(sess.run(loss, feed_dict={x: x_data, y: y_data})))
            losses.append(sess.run(loss, feed_dict={x: x_data, y: y_data}))
            #uncomment to save weights for future use:
            """np.savez("xavweights_v{}".format(counter), dec_w1 = sess.run(dec_w1),
                     dec_b1 = sess.run(dec_b1),
                     dec_w2 = sess.run(dec_w2),
                     dec_b2 = sess.run(dec_b2),
                     x_w_hat = sess.run(x_w_hat),
                     x_b_hat = sess.run(x_b_hat),
                     enc_w1 = sess.run(enc_w1),
                     enc_b1 = sess.run(enc_b1),
                     enc_w2 = sess.run(enc_w2),
                     enc_b2 = sess.run(enc_b2),
                     mu_hat = sess.run(mu_hat),
                     mu_b_hat = sess.run(mu_b_hat),
                     log_sigma_hat = sess.run(log_sigma_hat),
                     log_sigma_b_hat = sess.run(log_sigma_b_hat))
            print("Saving weights to filename xavweights_v{}".format(counter))"""
        if np.mod(i,500) == 0:
            print("Step: {} | Reconstruction loss: {} | KL divergence: {} ".format(
                 i,(sess.run(tf.reduce_sum(recon_loss),feed_dict={x: x_data, y: y_data})),
                 (sess.run(tf.reduce_sum(kl),feed_dict={x: x_data, y: y_data}))))
    #uncomment if saving weights:
    """
    xdec_weights = dict([('dec_w1',sess.run(dec_w1)),
                   ('dec_b1',sess.run(dec_b1)),
                   ('dec_w2',sess.run(dec_w2)),
                   ('dec_b2',sess.run(dec_b2)),
                   ('x_w_hat',sess.run(x_w_hat)),
                   ('x_b_hat',sess.run(x_b_hat))])
    xenc_weights = dict([('enc_w1',sess.run(enc_w1)),
                    ('enc_b1',sess.run(enc_b1)),
                    ('enc_w2',sess.run(enc_w2)),
                    ('enc_b2',sess.run(enc_b2)),
                    ('mu_hat',sess.run(mu_hat)),
                    ('mu_b_hat',sess.run(mu_b_hat)),
                    ('log_sigma_hat',sess.run(log_sigma_hat)),
                    ('log_sigma_b_hat',sess.run(log_sigma_b_hat))])
    """

    #test
    n = 3
    input_x = np.empty((28*n,28*n))
    output_x = np.empty((28*n,28*n))
    
    for i in range(n):
        x_data,y_data = gen_data(batch_size)
        test = sess.run(x_hat, feed_dict={x: x_data, y: y_data})
        
        #originals
        for j in range(n):
            x_data = x_data[:,:784]
            input_x[i*28:(i+1)*28,j*28:(j+1)*28] = \
                x_data[j].reshape([28,28])
        
        #reconstructions
        for j in range(n):
            test = test[:,:784]
            output_x[i*28:(i+1)*28,j*28:(j+1)*28] = \
                test[j].reshape([28, 28])
 
#visualize reconstructed images
plt.figure(figsize=(3, 3))
plt.imshow(input_x, origin="upper", cmap="gray")
plt.title("Original Images")
plt.show()

plt.figure(figsize=(3, 3))
plt.imshow(output_x, origin="upper", cmap="gray")
plt.title("Reconstructed Images")
plt.show()

plt.plot(losses)
plt.title("Loss")

plt.show()
