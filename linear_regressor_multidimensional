import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt

"""
Goal: generate a random slope (w) and bias (b) that we will make
random data points conform to. Define global variables batch_size
and features which will be the dimensions of later matrices.
"""
batch_size = 128
features = 5
w = np.random.randn(features,1) #w is a matrix now
b = np.random.randn() #set b to a random scalar

"""
Generate random x values that we will later make conform to our w.
Note that the y values are contingent upon the x values, so we first
initialize x as an array of size (batch_size,features) populated with random
values. Each corresponding y value is calculated by multiplying the slope
by the values of x and adding b. Finally we return both arrays for later
use.
"""
def gen_data(w, b, batch_size, features):
    x_data = np.random.rand(batch_size,features)
    y_data = np.dot(x_data,w)+b #np.dot for matrix multiplication
    return x_data,y_data
    
"""
Here we're just printing out w and b for later reference and the output of
our data generation function to make sure it came out as anticipated.
"""
print(w,b)
print(gen_data(w, b, batch_size, features))

"""
Now we're building the linear regression. Define the formula for calculating
y_hat and the mean standard error, which we're using as our loss function.
Use GradientDescentOptimizer to minimize the mean squared loss and define the 
learning rate. Finally, we instantiate the global initializer object so that 
we can use it in our Session.
"""
tf.reset_default_graph()

x = tf.placeholder(tf.float32, shape=(batch_size,features))
y = tf.placeholder(tf.float32, shape=(batch_size,1))

w_hat = tf.get_variable("w_hat", shape=(features,1), initializer=tf.random_normal_initializer(0,0.5))
b_hat = tf.get_variable("b_hat", shape=(1,1), initializer=tf.random_normal_initializer(0,0.5))

y_hat = tf.add(tf.matmul(x,w_hat),b_hat)

MSE = tf.reduce_mean(tf.square(y_hat-y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(MSE)

init = tf.global_variables_initializer()

"""
Perform the linear regression via a for loop for 1000 steps. For each iteration of
the loop, we run our data generation function, store the output of the optimizer
into an underscore variable, and print the loss. We feed the data into the
optimizer using feed_dict and putting our randomly generated values into the
x and y placeholder values we made earlier. Finally we print w_hat and b_hat (our
generated values) and w and b so that we can compare them and see how well the
regression performed.
"""
with tf.Session() as sess:
    sess.run(init)
    for i in range(10000):
        x_data, y_data = gen_data(w,b,batch_size,features)
        _ = sess.run(optimizer, feed_dict={x: x_data, y: y_data})
        if np.mod(i,500) == 0:
            print(sess.run(MSE, feed_dict={x: x_data, y: y_data}))
    print(sess.run([w_hat, b_hat]))
    print(w,b)
